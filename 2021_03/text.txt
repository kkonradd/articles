Tytuł:
Czym (nie) są sztuczne sieci neuronowe?

Streszczenie:
Sztuczne sieci neuronowe przez wielu odbierane są jako szczytowe osiągniecie w dziedzinie sztucznej inteligencji. Dzieje się tak za sprawą wszechobecnie występujących terminów DNN (Deep Neural Network) oraz CNN (Convolutional Neural Network) pojawiających się przy okazji prasowych doniesień o kolejnych sukcesach naukowców dążących do uzyskania inteligentnych algorytmów i urządzeń. Tak zwane Neural Engine i mechanizmy AI wspierające przetwarzanie danych, popularnie występujące np. w procesie przetwarzania obrazów cyfrowych są stałym elementem specyfikacji współczesnych telefonów i komputerów.

Prowadzi to do złudnego przekonania, iż sztuczna inteligencja jest faktem i możemy jej zaufać powierzając jej nasze życie czy to w pojeździe autonomicznym czy diagnozie i leczeniu schorzeń. Niestety mało osób wie, że obraz jest bardzo jednostronny. Wraz z doniesieniami o sukcesach nie przekazuje się kluczowych informacji związanych z funkcjonowaniem sztucznych sieci neuronowych. Mało kto głośno mówi, że w zasadzie są one czarną skrzynką, o której działaniu nic nie wiemy i raczej nie będziemy mogli zbyt dużo powiedzieć. Mimo ogromnego rozgłosu i popularności za głębokim uczeniem nie stoi zbyt wiele teorii. Jeśli coś nie działa, trudno jest zrozumieć, dlaczego. Cała dziedzina jest nadal bardzo empiryczna i opiera się głownie na metodzie prób i błędów.

W artykule opiszemy podstawy funkcjonowania sztucznych sieci neuronowych. Przedstawimy architekturę sieci konwolucyjnych i głębokich. Przedstawimy pola na których sieci te odnoszą sukcesy pokazując jednocześnie jak, dokładnie w tych samych zastosowaniach, łatwo je "oszukać". Zwrócimy uwagę na niebezpieczeństwa jakie z tym się wiążą. Spróbujemy nakreślić przyszłe kierunki w których zapewne sieci będą podążać i zaprezentujemy podejmowane w tym temacie badania naukowe.


Section: Wstęp
Głębokie sieci neuronowe (ang. Deep Neural Network, DNN), konwolucyjne sieci neuronowe (ang. Convolutional Neural Network, CNN) lub ogólniej: sztuczne sieci neuronowe a potocznie: sieci neuronowe — chyba każdy z nas zetknął się już z co najmniej z jednym z tych terminów, często w szerszym kontekście sztucznej inteligencji (ang. Artificial Intelligence) czy uczenia maszynowego (ang. Machine Learning). Sztuczne sieci neuronowe mają szczególne miejsce w dziedzinie sztucznej inteligencji ze względu na obszar jaki eksplorują. Ludzki mózg uznawany jest za ucieleśnienie inteligencji, przynajmniej tej jaką znamy na Ziemi. W dążeniu do skonstruowania tzw. sztucznej inteligencji nie dziwi zatem fakt "kopiowania" tego co przez tysiące lat wypracowała ewolucja.

Mimo tego co niektórzy twierdzą, nasze poznanie budowy mózgu, procesów w nim zachodzący i ich wzajemnych interakcji jest dopiero na początku i czeka ludzkość jeszcze długa droga zanim będziemy w stanie efektywnie, w czysto użytkowy sposób, z tego skorzystać. Wizja maszyn przejmujących kontrolę nad ludźmi niewątpliwie nie może być rozpatrywana w kategoria tylko fantastyki naukowej, ale być może kolejnego etapu ewolucji. Filmy takiej jak Matrix (1999) czy wcześniejszy Terminator (1984) lub dużo mniej znany Eagle Eye (2008) pokazują co może się stać, gdy maszyny przejmą kontrolę nad światem. W ich (maszyn) precyzyjnie określonym świecie, kierowanym określonymi regułami, człowiek jawi się jako szkodnik, pasożyt i zagrożenie dla planety [1]. Choć zaprzeczaliśmy temu, trudno jest nie zgodzić się z takim punktem widzenia — trudno zaprzeczyć temu, że nadmiernie eksplorujemy nasze środowisko, wyniszczamy się na wzajem, a zwrot "światem rządzi pieniądz i nic innego się nie liczy" nie jest tylko pustym sloganem. 

Z naukowego punktu widzenia interesujące jest nie tyle pytanie o to czy maszyny przejmą władzę nad ludźmi, ale czy maszyny przejmujące władzę nad ludźmi będą inteligentne, czy tylko będą inteligentnie się zachowywały. Można powiedzieć, że różnica nie jest wielka i jest to tylko gra słów. Językowo może i tak, ale na gruncie dziedziny nauki nazywanej sztuczną inteligencją jest to istotna różnica. Choć dla zewnętrznego obserwatora skutki będą takie same, to jednak przyczyny zupełnie inne, w czym jako ludzkość możemy upatrywać szans na przetrwanie dłużej niż prognozują to scenarzyści filmowi i autorzy książek.



Section: Czy jest się czego obawiać?
Nie udzielimy odpowiedzi na tytułowe pytanie — nie jest ona wcale łatwa i jednoznaczna. Zamiast tego przedstawimy fakty mogące przemawiać za odpowiedzią twierdzącą.

- Wspomaganie lekarzy w interpretacji skanów i próbek tkanek pod kontem diagnozowania określonych chorób [7]
- Autonomiczne pojazdy [8]
- Redukowanie kosztów energii dla centrów danych
- Tłumaczenia maszynowe, porównywalne z profesjonalnymi tłumaczami [15]
- Inteligentni wirtualni asystenci (Google Assistant, Alexa, Siri)
- Deepfake, czyli technika do nakładania obrazów, w tym również zamiana twarzy aktorów występujących w filmach [16]

Z przytoczonych powyżej przykładów można wyciągnąć wniosek, iż stopień w jakim opanowaliśmy symulowanie czynności mózgu, zakres zastosowania i realna "skuteczność" uprawnia użycie określenia "inteligencja" w stosunku do sztucznych sieci neuronowych.



Section: Jak zbudowana jest sztuczna sieć neuronowa?
Podstawowym elementem tworzącym sztuczną sieć neuronową jest neuron. Wzorowany na neuronie biologicznym, realizuje jego podstawową funkcjonalność jako element reagujący w wyniku odebrania sumarycznego pobudzenia przekraczającego pewną progową wartość. Pierwszy model sztucznego neuronu został przedstawiony już w latach 40 ubiegłego wieku. Sygnały wejściowe są wprowadzane do odpowiedników biologicznych dendrytów. Ich wartości początkowo przyjmowały stany binarne, co odnosiło się do biologicznego wystąpienia impulsu w danej chwili lub jego braku. Wyznaczanie sumarycznego pobudzenia polega na pomnożeniu wejść przez ich wagi, a następnie ich zsumowaniu. Sztuczny neuron zmienia wartość wyjściową po przekroczeniu progu aktywacji (lub ogólniej mówiąc funkcji aktywacji). Łącząc neurony ze sobą możemy tworzyć sieci, które w zależności od konfiguracji mają różne właściwości. Rozróżniamy warstwy, wewnątrz których neurony pracują niezależnie od siebie, przekazując dalsze informacje kolejnym warstwom (przez połączenia z parametrami, tj. wagami) lub są już końcowym wyjściem z sieci. 

Najprostsza sieć neuronowa jest zbudowana z jednej warstwy. Taka sieć realizuje liniową funkcję dyskryminacyjną (klasyfikację), czyli może określić do jakiej klasy należy wzorzec wejściowy. W przypadku sieci z jednym neuronem i dwoma wejściami granicą decyzyjną jest prosta. Dla większej ilości wejść, podział przestrzeni realizuje pewna hiperpłaszczyzna. Rozszerzenie takiego modelu o więcej neuronów prowadzi do uzyskania większej ilości wyjść z sieci, a tym samym możliwość klasyfikacji w większej ilości klas. Dalsza modyfikacja sieci polega na dodaniu tzw. warstwy ukrytej, czyli takiej, której wyjście jest wejściem dla kolejnej warstwy z neuronami i umożliwia nieliniową klasyfikację (lub aproksymację).

Kamieniem milowym w rozwoju sztucznych sieci neuronowych było opracowanie propagacji wstecznej, czyli algorytmu uczenia sieci neuronowych [14]. Dzięki wyznaczeniu błędów delta można modyfikować wartości wag połączeń w sieci. Obliczenia w procesie uczenia są wykonywane od końca, czyli od ostatniej warstwy w sieci, co jest kierunkiem przeciwnym do normalnego przepływu sygnałów podczas wyznaczania odpowiedzi. Ujmując matematycznie, jest to gradientowa metoda minimalizacji funkcji wielu zmiennych, więc wyjście z neuronu powinna wyznaczać różniczkowalna funkcję aktywacji.

Dane empiryczne dzieli się na poszczególne części – zbiór uczący, walidujący i testowy. Zbiór uczący służy do optymalizacji parametrów (wag) sieci. Zbiór walidujący pozwala dopracować strukturę modelu sieci na etapie uczenia. Zbiór testowy służy zaś do ostatecznego sprawdzenia, jak sieć radzi sobie z nieznanymi danymi po zakończeniu nauki. Poprawność działania sieci jest obliczana dla każdego z tych zbiorów oddzielnie. W zależności od charakteru sieci używana jest funkcja błędu, która umożliwia wyznaczenie odległości między pożądaną wartością pochodzącą ze zbioru danych, a odpowiedzią sieci.

Teoretycznie jest możliwość budowy sieci ze skończoną ilością neuronów w jednej warstwie ukrytej, która powinna dobrze aproksymować zbiór uczący [13], jednak w praktyce ilość neuronów powinna być wykładniczo duża. Co więcej, nie wiemy czy algorytm uczący daną sieć uzyska optymalne rozwiązanie. Alternatywnym podejściem jest dodanie większej ilości warstw. Przekształcanie początkowej reprezentacji danych na inne wersje w następujących po sobie warstwach modelu pozwala rozwiązywać złożone problemy.

Przetwarzanie danych w wielu krokach może zostać przedstawione na przykładzie rozpoznawania obrazu. Surowa reprezentacja takiego obiektu to macierz. Pierwszy etap może wykrywać krawędzie na zdjęciu przez porównywanie jasności sąsiednich elementów macierzy. Następny krok to detekcja wierzchołków, a dalej wykrywanie fragmentów specyficznych elementów. Ostatnia warstwa realizuje klasyfikator przedmiotów zawartych na obrazie. Przedstawiony pomysł to koncepcja deep learningu, czyli w szczególności głębokich sieci neuronowych DNN (ang. deep neural network). 

Głębokie uczenie stało się bardzo popularne, a równocześnie wzrosła też liczebność zbiorów i ilość danych na podstawie których można projektować model, a następnie go uczyć. Największą popularność zdobyły m.in. sieci konwolucyjne CNN (ang. convolutional neural network), implementujące operację splotu zamiast mnożenia macierzy wagowych. Sieci te wykorzystywane są do przetwarzania danych, których reprezentacja może być przedstawiona w postaci siatki, jednowymiarowej (np. szeregi czasowe) lub wielowymiarowej (np. obrazy).

Głębokie sieci neuronowe osiągnęły bardzo wysoką, czasem konkurującą z możliwościami człowieka, wydajność w wielu zadaniach widzenia komputerowego. W oparciu o te sukcesy coraz częściej są one wykorzystywane jako element składowe systemów sterujących w samochodami, bezzałogowymi statkami powietrznymi i robotami. Zauważmy jednakże, że głębokie sieci neuronowe nie wnoszą niczego nowego do teoretycznych podstaw budowy i działania sztucznych sieci neuronowych. W pewnym sensie znane były odkąd zaczęto używać wielowarstwowych sztucznych sieci neuronowych. Ich potencjalne możliwości także były znane już dawno temu. To, że dopiero w ostatnich latach przeżywają one swój rozkwit nie zawdzięczają żadnym wartym uwagi koncepcjom czy ideom, ale dostępności sprzętu, który jest zdolny do masowych obliczeń macierzowych (głównie karty graficzne), co umożliwiło ich wykorzystanie w praktyce.

Z matematycznego punktu widzenia, każda sieć neuronowa jest funkcją. Teoretycznie można by badać jej własności i przez to powiedzieć coś o działaniu konkretnej sieci. W praktyce, stopień złożoności takiej sieci (funkcji) jest tak duży, że jej analiza jest niezmiernie złożonym zagadnieniem. W konsekwencji zwykle jedyne co wiemy, to fakt przekształcania "w jakiś sposób" sygnałów wejściowych na sygnały wyjściowe. Niestety nie potrafimy zanalizować tego procesu bardziej szczegółowo. Przez to sztuczna sieć neuronowa określana jest mianem 
"czarnej skrzynki" wykonującej "magicznie" określone zadanie. Przynajmniej tak byśmy chcieli wierzyć. "Głównym twierdzeniem sztucznych sieci neuronowych jest to, że ucieleśniają one nowe i potężne ogólne zasady przetwarzania informacji. Te zasady są źle zdefiniowane. Często twierdzi się, że wyłaniają się one z samej sieci. Dzięki temu proste powiązanie statystyczne (podstawowa funkcja sztucznych sieci neuronowych) można opisać jako uczenie się lub rozpoznawanie. W 1997 roku Alexander Dewdney skomentował to mówiąc, że [...] rozwiązania znajdują się jak za dotknięciem czarodziejskiej różdżki" [2] "[...] można by stworzyć działającą sieć, nie rozumiejąc, jak ona działa: zbiór liczb, który opisuje jej zachowanie byłby według wszelkiego prawdopodobieństwa 'nieprzezroczystą, nieczytelną tabelą... bezwartościową jako zasób naukowy'". [2]



Section: A jednak nie zawsze działa
Czarnoskrzynkowa natura sztucznych sieci neuronach, przy ich jednoczesnej skuteczności w wielu zastosowaniach i łatwa dostępność za sprawą narzędzi typu Keras, prowadzi do rosnącego braku odpowiedzialności za to co sztuczne sieci neuronowe zrobią. Popularny dziś model postępowania w środowisku tzw. data scientist-ów, jak sami siebie określają ludzie zajmujący się na codzień pracą z danymi, jest taki, że w przypadku braku znanego postępowania bierze się narzędzie do trenowania sieci neuronowych i rozpoczyna z nim pracę. Praca ta ma charakter metodyki opartej o serię prób i błędów. Tak długo próbuje się zbudować odpowiednią sieć, kierując się doświadczeniem i intuicją, aż w pewnym momencie podczas testów uzyskamy zadowalająco niski poziom błędu. Wówczas uznajemy, że sieć jest nauczona i gotowa do pracy. I sieć zaczyna pracę. Jako element sterownika w samolocie, samochodzie narzędziu diagnostycznym czy decydującym o przyznaniu kredytu na mieszkanie.  Nie przypadkowo napisaliśmy powyżej o intuicji i doświadczeniu, gdyż nie ma metod pozwalających "wyliczyć" sztuczną sieć neuronową. W zasadzie, to o "gotowej" sieci nic nie potrafimy powiedzieć. Może z wyjątkiem stwierdzenia, że na danych testowych działa. To jednak nie jest żadnym dowodem. Każdy matematyk wie, że aby obalić pewne twierdzenie, wystarczy wskazać jeden przykład, dla którego ono nie działa. Aby jednak wykazać jego prawdziwość, trzeba wykazać jego działanie dla wszystkich możliwych przykładów. Jest to podstawowa prawda o której osoby korzystające ze sztucznych sieci neuronowych zapominają. Sankcjonują poprawność modelu zbudowanego w oparciu o sieć neuronową przez odpowiednio dużą liczbę przykładów potwierdzających jej działanie. Tym czasem każdy matematyk (a data scientist elementarną wiedzę w zakresie matematyki musi posiadać) wie, że... Zobaczmy, do czego takie lekceważące podejście prowadzi.

W pracy [4] autorzy podają algorytm pozwalający przeprowadzić skuteczny atak na głębokie sieci neuronowe przetwarzające dane wizyjne. Jako przykład podają obraz drogowego znaku stop oznaczającego bezwzględny nakaz zatrzymania auta, zwykle umieszczany w miejscach niebezpiecznych, gdzie wymagane jest ustąpienie pierwszeństwa przejazdu. Okazuje się, że naklejenie na znak kilku małych naklejek w postaci białych i czarnych prostokątów, zupełnie nieistotnych z punktu widzenia człowieka, powoduje, że bezbłędnie działająca do tej pory sieć, rozpoznaje zamiast znaku stop, znak ograniczający prędkość do 40 mil na godzinę (około 64 km/h). Zauważmy, że czasami podobne naklejki pojawiają się na znakach drogowych i zwykle umieszczane są przez kibiców piłkarskich manifestujących w ten sposób swój stosunek do drużyn piłkarskich. Jak pokazuje opisany eksperyment, ta z pozoru niewinna zabawa, może mieć dramatyczne konsekwencje w przypadku np. autonomicznego pojazdu, który zamiast zatrzymać się, beztrosko wjedzie na skrzyżowanie.

Autorzy pracy [5] podają metodę ataku na sieć neuronową identyfikującą twarze. Okazuje się, że wystarczy do czapki przykleić prostokątną karteczkę z kolorowym wzorem aby drastycznie zredukować możliwość rozpoznania twarzy. W ten sposób można by np. oszukać system monitorujący dostęp do pomieszczenia, który najzwyczajniej "nie zauważyłby" przechodzącej osoby.

W pracy [6] znajdujemy przykład pokazujący jak spreparować obraz aby system zobaczył coś, czego na nim nie ma (a przynajmniej, czego człowiek nie widzi) — w tym przypadku zamiast ulicy z pojazdami, system dostrzeże jedynie postać Minionka. W tej samej pracy pokazany jest także atak na system określający pozę człowieka a także system rozpoznający mowę. Ten ostatni błędnie rozpoznawał zmodyfikowane wypowiedzi, które dla człowieka wciąż brzmiały identycznie jak oryginały.

W artykule [9] podanych jest wiele przykładów błędnego działania systemów sztucznej inteligencji, np. błędne klasyfikowanie osób z astmą jako tych o niskim ryzyku wystąpienia zapalenia płuc.

Z kolei w [3] pokazano przykład takiego spreparowania obrazu, aby pomimo nieodróżnialności przez człowieka od oryginału leniwiec znajdujący się na drzewie był rozpoznawany jako bolid F1, a panda jako gibon.

Choć opisane eksperymenty dotyczą głównie obrazu, to zasady działania sieci są identyczne bez względu na wykonywane przez nie zadanie. Oznacza to, że podobnym atakom może ulec każda sztuczna sieć neuronowa.

Kolejnym przykładem jest sieć neuronowa służąca do rozpoznawania mowy. Autorzy pracy [10] poddawali manipulacji nagrania dźwiękowe. Różnica pomiędzy oryginalnymi nagraniami i odpowiednio spreparowanymi nie była zauważalna dla człowieka, ale doprowadziła do uzyskania całkowicie odmiennej transkrypcji wymawianych słów.

Innym rodzajem ataku jest zatruwanie danych (ang. data poisoning), które służą do uczenia sieci neuronowych. W roku 2016 powstał chatbot Tay opracowany przez firmę Microsoft do którego każdy internauta mógł napisać wiadomość przez portal Twitter. Nauka bota nie zakończyła się jednak wraz z jego udostępnieniem dla szerokiej publiczności w internecie, a była kontynuowana na podstawie dalszych interakcji z ludźmi. Część użytkowników rozpoczęła tak zwany "atak troli" polegający na przesyłaniu wiadomości, które były niepoprawne politycznie lub były sprzeczne z rzeczywistymi faktami. Po zaledwie 16 godzinach Tay przesłał ponad 96000 wiadomości i został zawieszony, ponieważ nauczył się naśladować zachowania ofensywnych użytkowników [11]. Nie jest do końca jasne, jak wyglądała architektura, która była podstawą działaniem bota. Wiadome jest jednak, że problem zatrutych danych może dotknąć sieci neuronowe, które uczą się także w trakcie działania, na nowo pozyskanych danych.


Section: Konsekwencje
Poprzednią sekcję można by było zakończyć stwierdzeniem, że podane przykłady są marginalne, wydumane i zdarzają się z bardzo małym prawdopodobieństwem. Autorzy tej pracy nie chcieli by jednak być nigdy rozjechani przez auto którego kontroler ruchu sterowany przez sztuczną sieć neuronową mimo wszystko popełnił mało prawdopodobny błąd. Tak samo nie chcieliby być źle zdiagnozowani tylko dlatego, że ich przypadek chorobowy jest marginalny, albo samolot którym podróżowali spadł, gdyż jednak wstąpiły warunki pogodowe zaklasyfikowane wcześniej jako zbyt wydumane i z tego powodu niewystępujące w zbiorze testowym.

Jak możemy przeczytać w [3]:
'W swoich wysiłkach, aby dowiedzieć się, co jest nie tak [z głębokimi sieciami neuronowymi], naukowcy odkryli wiele powodów, dla których [one] zawodzą. "Nie ma sposobu naprawienia podstawowych [leżących u podstaw działania] słabości głębokich sieci neuronowych" — przekonuje François Chollet, inżynier AI w Google w Mountain View w Kalifornii.
[...]
"Każdy, kto bawił się uczeniem maszynowym wie, że te systemy od czasu do czasu popełniają głupie błędy", mówi Yoshua Bengio z University of Montreal w Kanadzie, który jest pionierem głębokiego uczenia się. "Niespodzianką [dla nas] był rodzaj błędu", mówi. "To było całkiem uderzające. To rodzaj błędu, którego nie wyobrażaliśmy sobie, że może się wydarzyć".
[...]
Sztuczna inteligencja wyszkolona w rozpoznawaniu samolotów może stwierdzić, że cechy, takie jak kolory obszarów, tekstury lub tła, są tak samo silnymi predyktorami, jak rzeczy, które [my ludzie] uznalibyśmy za istotne, takie jak skrzydła. Ale oznacza to również, że bardzo mała zmiana danych wejściowych może przekształcić je do postaci, którą sztuczna inteligencja potraktuje jako inny stan [w przeciwieństwie do człowieka, dla którego oba stany będą reprezentowały taką samą informację].'

Jak widać, można odpowiednio spreparować dane, które są nierozróżnialne od oryginału dla człowieka, ale znaczą zupełnie coś innego dla modelu sztucznej sieci neuronowej. Jeszcze większe możliwości daje manipulacja sieciami, które douczają się w trakcie działania. Mogą być one podatne na zorganizowane ataki, które celowo wprowadzają błędy lub zniekształcenia do modelu, którego parametry są na bieżąco aktualizowane.


Section: Czy możemy coś z tym zrobić?
Czy możemy coś z tym zrobić? Z pewnością możemy próbować. Problem polega na tym, że postęp nad badaniami związanymi ze sztucznymi sieciami neuronowymi jest niewspółmierny do znacznych zasobów poświęcanych na nie. Trudno wskazać istotne kamienie milowe przekroczone na drodze do lepszego ich wykorzystania. Obecnie wykorzystywany model neuronu zaproponowany został w roku 1943 i nie uległ od tego czasu żadnym zmianom. Możliwości są zatem dwie: albo model jest taki doskonały, albo ludzie nie potrafią go ulepszyć. Podstawowym zarzutem wysuwanym w kierunku sztucznych sieci neuronowych jest to, że nie odzwierciedlają one w wystarczającym stopniu faktycznego funkcjonowania neuronów. Propagacja wsteczna, choć jest istotnym narzędziem pozwalającym obecnie uczyć nam nasze sztuczne sieci neuronowe, to nie znajduje odpowiednika w biologicznych strukturach — taki mechanizm nie istnieje w biologicznych sieciach neuronowych. Nadal nie wiadomo, w jaki sposób informacje są kodowane przez prawdziwe neurony. Jest jeszcze inna możliwość: zostaliśmy uśpieni przez sukcesy sztucznych sieci neuronowych i sami przed sobą udajemy, nie chcemy dostrzec, ich wad. Zaznaczmy to raz jeszcze: poprawność działania sztucznej sieci neuronowej wykazana na zbiorze testowym, wykazuje tylko i wyłącznie poprawność jej działania na zbiorze testowym. Jakiekolwiek inne stwierdzenia nie posiadają umocowania w dorobku naukowym ludzkości. W tym kontekście nasze prace  wydają się wytyczać nowy i właściwy kierunek badań. Wzorując się na modelu uczenia maszynowego wprowadzonym przez Vladimira Vapnika [vapnik] rozważamy blok uczenia maszynowego jako zbiór funkcji. Ponieważ, jak zauważyliśmy to wcześniej, sztuczna sieć neuronowa jest funkcją (może być przez funkcję opisana), zatem elementami tego bloku mogą być sieci neuronowe. Stosując podejście z zakresu teorii optymalizacji do tego bloku możemy coś o nim wnioskować — mówiąc inaczej, możemy wnioskować coś o pewnych sieciach neuronowych. Jeśli tylko potrafimy opisać metodę konstruowania sieci w terminach matematycznych, wówczas możemy precyzyjnie określić jaka rodzina sieci neuronowych tworzy blok uczenia maszynowego. Dzięki temu możemy wnioskować coś o wszystkich sieciach neuronowych możliwych do uzyskania w ramach tej rodziny. Badając własności konkretnej rodziny, możemy określić na ile konkretna sieć z tej rodziny jest optymalna. Innymi słowy, możemy powiedzieć, czy możemy mieć sieć jeszcze lepszą czy jest to już niemożliwe bo obecna sieć jest najlepsza (optymalna) w ramach określonej rodziny sieci budowanych według pewnych ustalonych reguł. Zauważmy, że to podejście nie opiera się na tym co mamy dziś, czyli określaniu możliwości jednej wybranej sieci na podstawie przykładów uczących i testowych mających jakoby potwierdzić jej jakość. Zamiast tego dotykamy tego, czego ciągle nam brakuje w teorii sztucznych sieci neuronowych: odpowiedzi na pytanie czy sieć, działająca dobrze na danych testowych, zadziała tak samo dobrze na danych które dostarczymy jej kiedykolwiek w przyszłości. W ten sposób możemy "zmierzyć" i wyrazić za pomocą liczby podatność sieci na bycie "oszukaną" lub możliwość popełnienia błędu w przyszłości, nawet gdyby wszelkie dotychczasowe testy potwierdzały 100% skuteczność.


Section: Podsumowanie
Sztuczne sieci neuronowe są z pewnością efektywnym narzędziem wykorzystywanym gdy analityczne zbudowanie modelu jest bardzo trudne. Mogą znaleźć zastosowanie wszędzie tam, gdzie związek pomiędzy sygnałami wejściowymi a wyjściowymi jest trudny do opisania. Odpowiednio zbudowana i wytrenowana sieć takiej zależności może się nauczyć. Ich zalety nie mogą jednak przysłaniać wad, w tym największej, tj. niepewności. Funkcjonowanie na zasadzie "czarnej skrzynki" oznacza brak zrozumienia dlaczego sieć działa. W konsekwencji trudno jest także określić, kiedy nie zadziała. W przyszłości może to prowadzić do bardzo niebezpiecznych zdarzeń, kiedy to nauczona się, której ufamy, ale to zaufanie nie jest poparte żadnym matematycznym dowodem, popełni "błąd" -- zachowa się inaczej niż tego się spodziewaliśmy, ale jednakże zgodnie ze swoim modelem, który niestety pozostaje dla nas ukryty i nieznany.





Bibliografia:

[1]
53 Best Matrix Quotes That You Won't Want To Dodge
https://kidadl.com/articles/best-matrix-quotes-that-you-wont-want-to-dodge
“Human beings are a disease, cancer of this planet.”

– Agent Smith, ‘The Matrix' (1999).

Hugo Weaving: Agent Smith
https://www.imdb.com/title/tt0133093/characters/nm0915989
Agent Smith : I'd like to share a revelation that I've had during my time here. It came to me when I tried to classify your species and I realized that you're not actually mammals. Every mammal on this planet instinctively develops a natural equilibrium with the surrounding environment but you humans do not. You move to an area and you multiply and multiply until every natural resource is consumed and the only way you can survive is to spread to another area. There is another organism on this planet that follows the same pattern. Do you know what it is? A virus. Human beings are a disease, a cancer of this planet. You're a plague and we are the cure.

[2]
Artificial neural network
https://en.wikipedia.org/wiki/Artificial_neural_network

A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney commented that, as a result, artificial neural networks have a "something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything".

[vapnik]
Vapnik, V.
The Nature of Statistical Learning Theory. Springer New York (2000)


[4]

Kevin Eykholt∗1, Ivan Evtimov*2, Earlence Fernandes2, Bo Li3,
Amir Rahmati4, Chaowei Xiao1, Atul Prakash1, Tadayoshi Kohno2, and Dawn Song
Robust Physical-World Attacks on Deep Learning Visual Classification
https://arxiv.org/pdf/1707.08945.pdf

[5]
Advhat: Real-World Adversarial Attack On Arcface Face Id System
Stepan Komkov1,2, Aleksandr Petiushko
https://arxiv.org/pdf/1908.08705.pdf

[6]
Houdini: Fooling Deep Structured Prediction Models
Moustapha Cisse
Natalia Neverova*
Yossi Adi*
Joseph Keshet
https://arxiv.org/pdf/1707.05373.pdf


[3]
NEWS FEATURE
09 October 2019
Why deep-learning AIs are so easy to fool
Artificial-intelligence researchers are trying to fix the flaws of neural networks.
Douglas Heaven
https://www.nature.com/articles/d41586-019-03013-5
Nature 574, 163-166 (2019)
doi: https://doi.org/10.1038/d41586-019-03013-5

[7]
Neil Savage
Digital assistants aid disease diagnosis
Artificial intelligence could help clinicians to interpret scans and tissue samples
Nature 573, S98-S99 (2019)
25 September 2019
doi: https://doi.org/10.1038/d41586-019-02870-4
https://www.nature.com/articles/d41586-019-02870-4


[8]
Waldrop, M. Autonomous vehicles: No drivers required. Nature 518, 20–23 (2015). https://doi.org/10.1038/518020a
https://www.nature.com/articles/518020a

[9]
Crawford, K., Calo, R. There is a blind spot in AI research. Nature 538, 311–313 (2016). https://doi.org/10.1038/538311a
https://www.nature.com/articles/538311a#citeas

[10]
Nicholas Carlini, David Wagner
Audio Adversarial Examples: Targeted Attacks on Speech-to-Text
https://arxiv.org/pdf/1801.01944.pdf

[11]
Trolls turned Tay, Microsoft’s fun millennial AI bot, into a genocidal maniac
https://www.washingtonpost.com/news/the-intersect/wp/2016/03/24/the-internet-turned-tay-microsofts-fun-millennial-ai-bot-into-a-genocidal-maniac/

[12] W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4):115–133, 1943.

[13] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks
are universal approximators. Neural Networks, 2(5):359 – 366, 1989.

[14] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 323(6088):533–536, 1986.

[15] 
Martin Popel, Marketa Tomkova, Jakub Tomek, Łukasz Kaiser, Jakob Uszkoreit, Ondřej Bojar & Zdeněk Žabokrtský 
Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals
https://www.nature.com/articles/s41467-020-18073-9

[16]
Thanh Thi Nguyen, Quoc Viet Hung Nguyen, Cuong M. Nguyen, Dung Nguyen, Duc Thanh Nguyen, Saeid Nahavandi
Deep Learning for Deepfakes Creation and Detection: A Survey
https://arxiv.org/pdf/1909.11573.pdf

=== INNE ===

Artificial neural network
https://en.wikipedia.org/wiki/Artificial_neural_network

22. “What is 'real'? How do you define 'real'? If you’re talking about what you can feel, what you can smell, taste and see then 'real' is simply electrical signals interpreted by your brain.”

- Morpheus, ‘The Matrix' (1999).




